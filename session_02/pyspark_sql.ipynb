{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_sql.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7vRh7EGS30"
      },
      "source": [
        "## **PySpark SQL - Getting Started Notebook**\n",
        "This notebook provides an example for installing Spark dependencies and a simple \"getting started\" syntax for PySpark SQL.\n",
        "\n",
        "NOTE: This notebook is designed to be used with Google's Colab notebook and the Python 3 runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ24BRz6FMwN"
      },
      "source": [
        "## **Install Spark Dependencies**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upqpisH2IoMy"
      },
      "source": [
        "# Install Spark dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget --no-cookies --no-check-certificate https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar zxvf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2xvv-QnsQZs"
      },
      "source": [
        "!ls -al | grep spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NgWpb22FchD"
      },
      "source": [
        "## **Set env variables within Pyspark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I47MHeUcyH1j"
      },
      "source": [
        "# Set up required environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wa2iydLFsWW"
      },
      "source": [
        "## **Download Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYe4U8Jnx39a"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/zaratsian/iaa-2022/main/session_02/bikeshare_station_info.csv\n",
        "!wget https://raw.githubusercontent.com/zaratsian/iaa-2022/main/session_02/bikeshare_trips.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvMtRJwUFzie"
      },
      "source": [
        "## **Import Python and PySpark Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-SIpC_-aw0t"
      },
      "source": [
        "import datetime\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import monotonically_increasing_id, col, expr, when, concat, lit, udf, split\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.regression import GBTRegressor, LinearRegression, GeneralizedLinearRegression\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyT917EuF7Pv"
      },
      "source": [
        "## **Initialize Spark Session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niAz2S672M_m"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"Bikesharing SparkSQL\").master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL4NPf_ZF_OF"
      },
      "source": [
        "## **Read CSV into Spark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC6C3wKwyBbt"
      },
      "source": [
        "bikeshare_trips = spark.read.load('bikeshare_trips.csv', format=\"csv\", header=True, inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqf2c9xOGFDb"
      },
      "source": [
        "## **Display first few records**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deym0a0pyNNu"
      },
      "source": [
        "bikeshare_trips.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_v2cyCFGMn1"
      },
      "source": [
        "## **Execute Sample SparkSQL query**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPheTgdLyddA"
      },
      "source": [
        "bikeshare_trips.createOrReplaceTempView(\"bikeshare_trips\")\n",
        "spark.sql(\"SELECT subscriber_type, count(*) as count FROM bikeshare_trips group by subscriber_type order by count desc\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nEWC_7BUzNKN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}